{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "import time\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"download dataset\"\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "if not os.path.isfile(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the data is 1115394\n"
     ]
    }
   ],
   "source": [
    "data = open(file_name).read() # This contains all the file in a string in memory\n",
    "print(\"the size of the data is {}\".format(len(data)))\n",
    "vocab = set(data) # upper-case and lower-case characters are different\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_data = list(map(lambda x: vocab_to_idx[x], data))\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_epochs(num_epochs, batch_size, num_steps):\n",
    "    for _ in range(num_epochs):\n",
    "        yield reader.ptb_iterator(numeric_data, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the previously written code to build a language model\n",
    "from basic_rnn_using_tensorflow_api import BasicRNN\n",
    "from basic_lstm_using_tensorflow_api import BasicLSTM\n",
    "from basic_lstm_using_dynamicRNN import DynamicLSTM\n",
    "from basic_lstm_using_tfScan import DynamicScannedLSTM\n",
    "from basic_lstm_using_custom_gru import CustomGRU\n",
    "from basic_lstm_using_layer_normalized_lstm import CustomLSTM\n",
    "from basic_GRU_using_dynamicRNN import DynamicGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "batch_size = 32\n",
    "num_steps = 200\n",
    "num_classes = vocab_size\n",
    "inlayer_dropout = 0.6\n",
    "num_weights = 3\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN with one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the basic RNN model from list is  34.41115379333496\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model = BasicRNN(state_size=state_size, num_steps=num_steps, batch_size=batch_size,\n",
    "                          num_classes=num_classes, num_layers=1, inlayer_dropout=inlayer_dropout,\n",
    "                          learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the basic RNN model from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.72\n",
      "loss after 1 epoch is 3.36\n",
      "loss after 2 epoch is 3.23\n",
      "time taken to finish this simulatin is 1.00 minutes\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN with 3-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the basic RNN model with 3 layers from list is  81.43449544906616\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_3_layer_rnn = BasicRNN(state_size=state_size, num_steps=num_steps, num_layers=3, \n",
    "                                      batch_size=batch_size, inlayer_dropout=inlayer_dropout,\n",
    "                                      num_classes=num_classes, learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the basic RNN model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.67\n",
      "loss after 1 epoch is 3.24\n",
      "loss after 2 epoch is 3.02\n",
      "time taken to finish this simulatin is 1.66\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_3_layer_rnn.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LSTM with 3 layers using tf.rnn api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the basic LSTM model with 3 layers from list is  148.90693283081055\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_3_layer_lstm = BasicLSTM(state_size=state_size, num_steps=num_steps, \n",
    "                                        num_layers=3, batch_size=batch_size, inlayer_dropout=inlayer_dropout,\n",
    "                                        num_classes=num_classes, learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the basic LSTM model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.70\n",
      "loss after 1 epoch is 3.45\n",
      "loss after 2 epoch is 3.41\n",
      "time taken to finish this simulatin is 5.75\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_3_layer_lstm.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is taking lot of time to just build the model. This is not a problem during training time because we have to build the model only once. But it could be a problem during test time where we may have to build the model multiple times. We can use a `Tensorflow` api `DynamicRNN` that can delay the creation of the graph to the run time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LSTM using tf.dynamic_rnn api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the dynamic LSTM model with 3 layers from list is  1.6791408061981201\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_dynamic_lstm = DynamicLSTM(state_size=state_size, num_steps=num_steps, num_layers=3, \n",
    "                                         batch_size=batch_size, num_classes=num_classes, \n",
    "                                          inlayer_dropout=inlayer_dropout, learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the dynamic LSTM model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.58\n",
      "loss after 1 epoch is 3.35\n",
      "loss after 2 epoch is 3.33\n",
      "time taken to finish this simulation is 4.64\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_dynamic_lstm.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LSTM using tf.scan api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the dynamic LSTM model with 3 layers from list is  1.8496365547180176\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_scanned_lstm = DynamicScannedLSTM(state_size=state_size, num_steps=num_steps, num_layers=3, \n",
    "                                                 batch_size=batch_size, num_classes=num_classes, \n",
    "                                                 inlayer_dropout=inlayer_dropout,\n",
    "                                                 learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the dynamic LSTM model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.61\n",
      "loss after 1 epoch is 3.35\n",
      "loss after 2 epoch is 3.34\n",
      "time taken to finish this simulation is 4.63\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_scanned_lstm.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Gated Recurrent Unit (GRU) using 3 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, I have created a custom GRU unit. This GRU unit uses $n$ weights. This Custom GRU is based on the follwoing intuitions:\n",
    "1. Each sentence has subject, verb, and object.\n",
    "2. We should treat subject, verb, and object differently.\n",
    "3. We would learn the way weights should be treated by subject, verb, and object dynamically using $\\lambda_i$s.\n",
    "4. The input that would go to RNN will be $\\sum_i \\lambda_i W_i x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the dynamic LSTM model with 3 layers from list is  3.9994800090789795\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_custom_gru = CustomGRU(state_size=state_size, num_steps=num_steps, num_layers=3, \n",
    "                                                 batch_size=batch_size, num_classes=num_classes, \n",
    "                                                 inlayer_dropout=inlayer_dropout, num_weights=num_weights,\n",
    "                                                 learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the dynamic LSTM model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.49\n",
      "loss after 1 epoch is 2.90\n",
      "loss after 2 epoch is 2.54\n",
      "time taken to finish this simulation is 7.42\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_custom_gru.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalized LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Normalization helps in stable and fast learning for RNN as Batch Normalization works for Feed Forward Networks. Layer Normalization works as following:\n",
    "1. We layer normalized input that goes to a non-linearity.\n",
    "2. By a given input, we compute the hiddent activation that it casued. For example, in RNN, at time $t$, when input $x_t$ and previous state $h_{t - 1}$ goes to a cell, it is first transformed to $z_t = W_t [x_t, h_t] + b_t$. In layer normalization, we normalize $z_t$. Essentially, we transform $z_t$ to \n",
    "$$\n",
    "\\tilde{z}_t = \\gamma_t \\frac{(z_t - mean(z_t))}{Var(z_t)} + \\beta_t.\n",
    "$$\n",
    "Subsequently, we feed this normalized input to the non-linearity and compute the next state \n",
    "$$h_t = \\tanh{\\tilde{z}_t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the dynamic LSTM model with 3 layers from list is  9.066001176834106\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_custom_LSTM = CustomLSTM(state_size=state_size, num_steps=num_steps, num_layers=3, \n",
    "                                                 batch_size=batch_size, num_classes=num_classes, \n",
    "                                                 inlayer_dropout=inlayer_dropout,\n",
    "                                                 learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the dynamic LSTM model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.71\n",
      "loss after 1 epoch is 3.38\n",
      "loss after 2 epoch is 3.33\n",
      "time taken to finish this simulation is 13.89 minutes\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(3, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_custom_LSTM.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a gated recurrent unit for 20 epochs, save the model, and use it for generating texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to build the dynamic LSTM model with 3 layers from list is  2.747432231903076\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_dynamic_gru = DynamicGRU(state_size=state_size, num_steps=num_steps, num_layers=3, \n",
    "                                                 batch_size=batch_size, num_classes=num_classes, \n",
    "                                                 inlayer_dropout=inlayer_dropout,\n",
    "                                                 learning_rate=learning_rate)\n",
    "toc = time.time()\n",
    "print(\"The time took to build the dynamic LSTM model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 3.60\n",
      "loss after 1 epoch is 3.35\n",
      "loss after 2 epoch is 3.33\n",
      "loss after 3 epoch is 3.26\n",
      "loss after 4 epoch is 3.07\n",
      "loss after 5 epoch is 2.89\n",
      "loss after 6 epoch is 2.79\n",
      "loss after 7 epoch is 2.72\n",
      "loss after 8 epoch is 2.65\n",
      "loss after 9 epoch is 2.58\n",
      "loss after 10 epoch is 2.49\n",
      "loss after 11 epoch is 2.43\n",
      "loss after 12 epoch is 2.38\n",
      "loss after 13 epoch is 2.33\n",
      "loss after 14 epoch is 2.29\n",
      "loss after 15 epoch is 2.26\n",
      "loss after 16 epoch is 2.23\n",
      "loss after 17 epoch is 2.21\n",
      "loss after 18 epoch is 2.18\n",
      "loss after 19 epoch is 2.16\n",
      "time taken to finish this simulation is 31.08 minutes\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for n_epoch, epoch in enumerate(gen_epochs(20, batch_size, num_steps)):\n",
    "    loss, step = 0, 0\n",
    "    for batch in epoch:\n",
    "        loss += language_model_dynamic_gru.update_params(batch)\n",
    "        step += 1\n",
    "    print(\"loss after {0} epoch is {1:0.2f}\".format(n_epoch, loss / step))\n",
    "toc = time.time()\n",
    "print(\"time taken to finish this simulation is {0:0.2f} minutes\".format((toc - tic) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restoring the previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time took to restore the dynamic GRU model with 3 layers from list is  9.898928165435791\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tic = time.time()\n",
    "language_model_dynamic_gru = DynamicGRU(state_size=state_size, num_steps=1, num_layers=3, \n",
    "                                                 batch_size=1, num_classes=num_classes, \n",
    "                                                 inlayer_dropout=inlayer_dropout,\n",
    "                                                 learning_rate=learning_rate)\n",
    "language_model_dynamic_gru.saver.restore(language_model_dynamic_gru.session, \"dynamic_gru_model.ckpt-3400\")\n",
    "toc = time.time()\n",
    "print(\"The time took to restore the dynamic GRU model with 3 layers from list is \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_characters(initial_character=\"S\", num_chars=200, initial_state=None, choose=\"multinomial\"):\n",
    "    char = [initial_character]\n",
    "    lm = language_model_dynamic_gru\n",
    "    input_ = [[vocab_to_idx[initial_character]]]\n",
    "    for _ in range(num_chars):\n",
    "        lm.init_state, probs = lm.session.run([lm.final_state, lm.probs], {lm.input: input_})  \n",
    "        if choose == \"multinomial\":\n",
    "            input_ = [[np.argmax(np.random.multinomial(1, probs[0]))]]\n",
    "        elif choose == \"max\":\n",
    "            input_ = [[np.argmax(probs[0])]]\n",
    "        char.append(idx_to_vocab[input_[0][0]])\n",
    "    return \"\".join(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te te t'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_characters(\"E\", choose=\"max\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
